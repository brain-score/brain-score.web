{% extends "benchmarks/components/app-view.html" %}
{% load static %}

{% block banner %}
<h1>Creating a Benchmark</h1>
<p>How to create a Brain-Score benchmark with packaged data.</p>
{% endblock %}

{% block info_section %}
  {% include "benchmarks/tutorials/tutorial-info-section.html" %}
{% endblock %}


{% block content %}

<div class="box leaderboard-table-component">
    <div class="columns is-tablet is-variable is-1-tablet">
        <div class="column is-one-half">
            <img class="tutorial_dd1_1" src="{% static "/benchmarks/img/tutorials/deep_dive_1_1.png" %}" />
        </div>
        <div class="column is-one-half tutorial_text">
            <h3 class="benefits_heading is-size-3-mobile">Let's Create a Benchmark!</h3>
            <p class="benefits_info is-size-5-mobile">
                Welcome to the second half of our Benchmark tutorial, where we will cover how to actually create
                a benchmark, given you already have packaged your data. We will do this by again looking at a sample
                benchmark, Ferguson2024, and how it is structured.
            </p>
            <p class="benefits_info is-size-5-mobile">
                This section has three main parts, corresponding to the three main files that make up a benchmark plugin.
                If you just packaged your data, then you probably can guess what some of these files are: there is a
                <span class="special_format">__init__.py</span> file, a <span class="special_format">test.py</span> file,
                and a new <span class="special_format">benchmark.py</span> file. Each will be explored in detail below.
            </p>
        </div>
    </div>
</div>

<div class="box leaderboard-table-component">
    <div class="columns is-tablet is-variable is-1-tablet">
        <div class="column is-one-half">
            <h3 class="benefits_heading is-size-3-mobile">Part 1: Install Necessary Packages</h3>
            <p class="benefits_info is-size-5-mobile">
                If you have not already, we highly recommend completing the 'Package Data' section before
                beginning this section. Although completion is not strictly required, the Package Data section will give
                you a nice background into the stimuli and data side of a Brain-Score benchmark.
            </p>
            <p class="benefits_info is-size-5-mobile">
                If you have already done the Package Data tutorial,
                skip to step 2 to the right. Otherwise, visit please <a href="https://www.brain-score.org/tutorials/benchmarks/package_data">here</a>
                to install the necessary packages for Brain-Score and to learn about stimuli and data packaging.
            </p>
        </div>
        <div class="column is-one-half">
            <h3 class="benefits_heading is-size-3-mobile">Part 2: Access the Sample Data Folder:</h3>
            <p class="benefits_info is-size-5-mobile">
                As was the case for the package Data tutorial, we will be looking at a sample benchmark, Ferguson2024,
                in order to understand the many parts of a benchmark plugin. You can view our stock submission in your local copy
                of Brain-Score that you cloned in Part 1;the folder we are going to be exploring is
                <span class="special_format">vision/brainscore_vision/benchmarks/ferguson2024</span>. If you cannot access
                a local copy of Brain-Score for some reason, the sample data folder is available on our Github
                <a href="https://github.com/brain-score/vision/tree/master/brainscore_vision/benchmarks/ferguson2024">here</a>.
            </p>
             <p class="benefits_info is-size-5-mobile">
                 Currently (as of July 2024), benchmark submission via the website are a little buggy. Once you have your
                 submission ready to go, reach out to Brain-Score Team member and we will submit your benchmark for you
                 via a Github PR.
            </p>
        </div>
    </div>
</div>

<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 3: Exploring the Sample Submission Folder</h3>
    <p class="benefits_info is-size-5-mobile shorter">
        If you completed the "Package Data" tutorial, this folder should look very familiar. It has all the parts needed
        of standard Brain-Score plugin (remember: a plugin can be a benchmark, a model, a metric, or data - we took a look
        at a data plugin previously in the first part of this tutorial series). In this folder you can see  an
        <span class="special_format">__init__.py</span>
        file for adding your benchmark plugin to Brain-Score's registry, a <span class="special_format">requirements.txt</span>
        file for any dependencies your benchmark might need, a <span class="special_format">test.py</span> file to create
        certain tests for your benchmark, and finally a <span class="special_format">benchmark.py</span> file that
        actually contains the code to run models on your stimuli and compare it to your data.
    </p>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_sample.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        There is also an extra folder, called <span class="special_format">helpers</span>, that contains an additional
        python file <span class="special_format">helpers.py</span>. This is not needed for every benchmark, but contains
        helper functions that the benchmark calls, including methods like subject statistics and metric calculators. We
        will largely be ignoring this file, but if your hypothetical benchmark required preprocessing functions, human data
        processing functions, or anything else, this is a good way to abstract them away from the main <span class="special_format">benchmark.py</span>
        file, but again this is not strictly needed.
    </p>
    <p class="benefits_info is-size-5-mobile shorter is-italic">
        Note: this would also be the same overall structure as your submission
        package, or if you are submitting any other plugins (models, metrics, or data). Feel free
        to base your future submissions off of this package.
    </p>
</div>

<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 4: Exploring the  __init__.py  file</h3>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_init.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        This file is basically the same as its cousin file we looked at in Package Data - it adds a benchmark, for
        example <span class="special_format">Ferguson2024circle_line-value_delta</span>, to the Brain-Score registry via
        the dictionary on lines 9-22. For a benchmark plugin, the key in the dictionary is the benchmark name, and its
        value is a Benchmark object, which we will see soon.
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        You can see here that there are 14 benchmarks that are being added to the registry, and in the case of our
        <span class="special_format">Ferguson2024</span> benchmark, these are 14 "sub benchmarks" that each use a different
        <span class="special_format">StimulusSet</span> and <span class="special_format">BehavioralAssembly</span>
        (explained in the previous half of this tutorial series). These 14 benchmarks belong to a parent benchmark object,
        <span class="special_format">Ferguson2024</span>, and are called via the 14 lambda expressions with a different
        stimuli and data identifier. We also try to follow
        the naming convention of <span class="special_format">AuthorYEAR-metric</span>, as shown here.
    </p>

    <p class="benefits_info is-size-5-mobile shorter">
        Please note: Brain-Score does not allow duplicate plugin
        names, so if you submit another version of the same model, make sure to make the identifier unique!
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Also note: It would be prohibitively time- and resource-consuming to actually load every plugin
        in a registry before they are needed, so the plugin discovery process relies on string parsing.
        This unfortunately means that it’s not possible to programmatically add plugin identifiers to
        the registry; each registration needs to be written explicitly in the form of
        <span class="special_format">plugin_registry['my_plugin_identifier']</span>
        Next, let’s check out the second required file, <span class="special_format">test.py</span> file.
    </p>
</div>
<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 5: Exploring the test.py File</h3>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_test.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        Our <span class="special_format">test.py</span> file again serves the purpose of testing our benchmark to ensure
        scores are coming out as expected. There is a little bit of a circular dependency here, as you will not know
        scores until you actually run your benchmark; however, you can run 1-2 models locally, and hardcode what the values
        should be in your tests, which is what is done in the screenshot above.
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        On lines 58-71, you can see that each of the 14 benchmarks have an expected raw score associated with them that
        is passed into a pytest object. These scores were generated via a local call to Brain-Score, and you can learn more
        about how to do that by visiting the model tutorial
        <a href="http{% if request.is_secure %}s{% endif %}://{{ request.get_host }}/tutorials/models/quickstart">Quickstart</a>.

    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Line 73 defines the function <span class="special_format">test_model_raw_score</span>, and that is what is seen
        in lines 74-85; we are loading a benchmark and a precomputed model's features on that benchmark, and them comparing
        that to an expected score. Note: to do this successfully requires uploading precomputed model features to our S3
        bucket (however you can also host externally); please contact a Team Member if you need credentials to do this. For
        more information on how to generate precomputed model features for your tests, please see
        <a href="https://github.com/brain-score/vision/tree/master/tests#precomputed-features">here</a> (The
        "Precomputed features" section, and you can ignore steps 5 and 6 that are now outdated).
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        There are also other tests that are in the <span class="special_format">test.py</span> that are simply not shown
        in the screenshot above. These include tests for existence of the benchmark, tests to ensure the benchmark ceiling
        is working correctly, and tests to make sure the ceiled score is what it should be. Please feel free to base
        future submission tests off of the tests in this file!
    </p>
</div>
<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 6: Exploring the (optional) requirements.txt  File</h3>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_req.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        As with all Brain-Score plugins, you can use a <span class="special_format">requirements.txt</span> to define
        any needed dependencies. If you do not include any external packages used here, such as numpy, sympy, and pandas,
        then your benchmark will not be able to run on our servers (this will be caught in Github's PR tests). The reason
        that this is optional is that if your plugin does not need any external packages, then you do not need to submit
        this file.
    </p>

</div>
<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 7: Exploring the benchmark.py File</h3>

    <p class="benefits_info is-size-5-mobile shorter">
        This file is arguably the most important file for the benchmark, as it defined how models will see stimuli, have
        their responses recorded, and compared to humans. There are many parts to this file, so we will look at a few
        lines at a time, starting with the benchmark class <span class="special_format">__init__</span>method.
    </p>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_init_2.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        The <span class="special_format">__init__</span>method shown above on lines 43-54 is what is called to initialize
        your benchmark. Lines 44-51 all set important hyperparameters: the metric, the visual degrees, the number of repetitions,
        and other fields that the benchmark uses. Lines 52-54 specifically call the <span class="special_format">super</span>
        constructor to tell Brain-Score that each of these benchmarks are a part of the parent benchmark
        <span class="special_format">Ferguson2024ValueDelta</span>
    </p>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_call.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        The <span class="special_format">__call__</span>method shown above has many important parts, and we will look at each
        one of them. In an nutshell, this method is what actually shows the model the stimuli, computes a model score, and
        compares that score with a human score via a metric to determine the final Brain-Score.
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Let's start with Lines 63 and 64: here the <span class="special_format">place_on_screen</span> Brain-Score
        method is called to transform your stimuli into visual degrees. The particular call on line 63 is for
        pretraining (transfer-learning) the models to force them to be able to do a binary task. For background, the
        Ferguson2024 benchmarks asked the subject to look at an image of objects, and return whether or not that image
        contains an "odd-one-out". So for models not trained explicitly to perform binary classification, this
        pretraining on line 63 and the decoder itself on line 64 force a binary output from the model, using a logistic
        regression decoder. Note: your benchmark might be completely different as far as the task goes. We have many
        other tasks supported in the Brain-Score, and you can always contribute a custom one. Lines 66 and 67 call the
        <span class="special_format">place_on_screen</span> method for the actual testing stimuli themselves.
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Next, line 68 is what actually shows the model the stimuli and returns a label for each stimuli, contained in this
       case inside the <span class="special_format">model_labels_raw</span> variable. Line 69 applies some post-processing
        on these labels in order to get them to be more human-readable, and if you were creating your own benchmark, you
        could probably ignore these.
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Lines 70 and 71 get the "human score" and the "model score" respectively. Again for context, the Ferguson2024
        benchmarks work by comparing a human integral of a specific graph with one generated by models. This is just one
        way to quantify human and model performance, and each benchmark has its own way of doing this. Line 72 is where
        these two scalars are passed into a metric in order to be compared. In this case, that metric is a simple
        value delta using a sigmoid curve. This metric takes in two scalars and returns a value between 0 and 1 based on
        how similar the two scalars are, but note that you can use any metric you wanted, or even create a custom metric.
        Please see <a href="https://github.com/brain-score/vision/tree/master/brainscore_vision/metrics">here</a> for a
        list of currently supported Brain-Score metrics, and note that many use human data directly (via assemblies) or
        a processed version (via a matrix of some description).
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Next comes the benchmark ceiling. A ceiling in this case represents the maximum score a perfect model could
        achieve on the data, and might not always be 1. Line 73 calls the ceiling method defined in lines 84-116, and simply
        splits the human data in half randomly and computes the metric on those two halves. Although you could choose any
        ceiling you wanted to, we strongly encourage you to use the exact same metric you used to compare models with
        subject data, as we believe this is a best practice. You can also precompute your ceilings, and we encourage this
        as well. You can see this reflected in lines 30-34, where a dictionary is used to store precomputed ceiling to
        save compute resources.
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Finally, lines 74-77 adjust the raw score by the ceiling, and then returns it. Also note that lines 80-81 define
        the function used in the benchmark's <span class="special_format">__init__.py</span> file, and if you are making
        your own benchmark, do not forget to include this. The rest of the lines after line 81 are extra helper functions
        for the benchmark, and can be ignored for our benchmark's sake.
    </p>
</div>
<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 8: Putting it All Together </h3>
    <p class="benefits_info is-size-5-mobile shorter">
       By now you have seen the heart of a benchmark, and hopefully feel comfortable enough with the benchmark plugin
        system that you could upload your own. We recognize that this might be a tedious process that is bound
        to raise questions, so please reach out to Team Member if you would like help getting your benchmark on our site.
        There is also another, slightly more in depth benchmark tutorial (that also covers neural data)
        <a href="https://brain-score.readthedocs.io/en/latest/modules/benchmark_tutorial.html">here</a> on our wiki.
    </p>

    <div class="columns is-tablet is-variable is-1-tablet">
        <div class="column is-one-half tutorial_text">
           <p class="benefits_info is-size-5-mobile shorter top_adjust">
               If you were submitting your own benchmark plugin, then after you make sure you have all the files
               that you need, you could send them to a Brain-Score team member and we would open a PR for you, where
               you can track progress on your contribution. Once the PR's tests pass and it is merged, your benchmark
               will be scored on the top 100 models!
           </p>
            <p class="benefits_info is-size-5-mobile shorter ">
               If you have any more questions, please see the benchmark section of our troubleshooting page
                <a href="http{% if request.is_secure %}s{% endif %}://{{ request.get_host }}/tutorials/troubleshooting">here</a>.
            </p>
        </div>
        <div class="column is-one-half">
            <img class="tutorial_dd1_2" src="{% static "/benchmarks/img/tutorials/deep_dive_1_2.png" %}" />
        </div>
    </div>


</div>
{% endblock %}


