{% extends "benchmarks/components/app-view.html" %}
{% load static %}

{% block banner %}
<h1>Creating a Benchmark</h1>
<p>How to create a Brain-Score benchmark with packaged data.</p>
{% endblock %}

{% block info_section %}
  {% include "benchmarks/tutorials/tutorial-info-section.html" %}
{% endblock %}


{% block content %}

<div class="box leaderboard-table-component">
    <div class="columns is-tablet is-variable is-1-tablet">
        <div class="column is-one-half">
            <img class="tutorial_dd1_1" src="{% static "/benchmarks/img/tutorials/deep_dive_1_1.png" %}" />
        </div>
        <div class="column is-one-half tutorial_text">
            <h3 class="benefits_heading is-size-3-mobile">Let's Create a Benchmark!</h3>
            <p class="benefits_info is-size-5-mobile">
                Welcome to the second half of our Benchmark tutorial, where we will cover how to actually create
                a benchmark, given you already have packaged your data. We will do this by again looking at a sample
                benchmark, Ferguson2024, and how it is structured.
            </p>
            <p class="benefits_info is-size-5-mobile">
                This section has three main parts, corresponding to the three main files that make up a benchmark plugin.
                If you just packaged your data, then you probably can guess what some of these files are: there is a
                <span class="special_format">__init__.py</span> file, a <span class="special_format">test.py</span> file,
                and a new <span class="special_format">benchmark.py</span> file. Each will be explored in detail below.
            </p>
        </div>
    </div>
</div>

<div class="box leaderboard-table-component">
    <div class="columns is-tablet is-variable is-1-tablet">
        <div class="column is-one-half">
            <h3 class="benefits_heading is-size-3-mobile">Part 1: Install Necessary Packages</h3>
            <p class="benefits_info is-size-5-mobile">
                If you have not already, we highly recommend completing the 'Package Data' section before
                beginning this section. Although completion is not strictly required, the Package Data section will give
                you a nice background into the stimuli and data side of a Brain-Score benchmark.
            </p>
            <p class="benefits_info is-size-5-mobile">
                If you have already done the Package Data tutorial,
                skip to step 2 to the right. Otherwise, visit please <a href="https://www.brain-score.org/tutorials/benchmarks/package_data">here</a>
                to install the necessary packages for Brain-Score and to learn about stimuli and data packaging.
            </p>
        </div>
        <div class="column is-one-half">
            <h3 class="benefits_heading is-size-3-mobile">Part 2: Access the Sample Data Folder:</h3>
            <p class="benefits_info is-size-5-mobile">
                As was the case for the package Data tutorial, we will be looking at a sample benchmark, Ferguson2024,
                in order to understand the many parts of a benchmark plugin. You can view our stock submission in your local copy
                of Brain-Score that you cloned in Part 1;the folder we are going to be exploring is
                <span class="special_format">vision/brainscore_vision/benchmarks/ferguson2024</span>. If you cannot access
                a local copy of Brain-Score for some reason, the sample data folder is available on our Github
                <a href="https://github.com/brain-score/vision/tree/master/brainscore_vision/benchmarks/ferguson2024">here</a>.
            </p>
             <p class="benefits_info is-size-5-mobile">
                 Currently (as of July 2024), benchmark submission via the website are a little buggy. Once you have your
                 submission ready to go, reach out to Brain-Score Team member and we will submit your benchmark for you
                 via a Github PR.
            </p>
        </div>
    </div>
</div>

<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 3: Exploring the Sample Submission Folder</h3>
    <p class="benefits_info is-size-5-mobile shorter">
        If you completed the "Package Data" tutorial, this folder should look very familiar. It has all the parts needed
        of standard Brain-Score plugin (remember: a plugin can be a benchmark, a model, a metric, or data - we took a look
        at a data plugin previously in the first part of this tutorial series). In this folder you can see  an
        <span class="special_format">__init__.py</span>
        file for adding your benchmark plugin to Brain-Score's registry, a <span class="special_format">requirements.txt</span>
        file for any dependencies your benchmark might need, a <span class="special_format">test.py</span> file to create
        certain tests for your benchmark, and finally a <span class="special_format">benchmark.py</span> file that
        actually contains the code to run models on your stimuli and compare it to your data.
    </p>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_sample.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        There is also an extra folder, called <span class="special_format">helpers</span>, that contains an additional
        python file <span class="special_format">helpers.py</span>. This is not needed for every benchmark, but contains
        helper functions that the benchmark calls, including methods like subject statistics and metric calculators. We
        will largely be ignoring this file, but if your hypothetical benchmark required preprocessing functions, human data
        processing functions, or anything else, this is a good way to abstract them away from the main <span class="special_format">benchmark.py</span>
        file, but again this is not strictly needed.
    </p>
    <p class="benefits_info is-size-5-mobile shorter is-italic">
        Note: this would also be the same overall structure as your submission
        package, or if you are submitting any other plugins (models, metrics, or data). Feel free
        to base your future submissions off of this package.
    </p>
</div>

<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 4: Exploring the  __init__.py  file</h3>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_init.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        This file is basically the same as its cousin file we looked at in Package Data - it adds a benchmark, for
        example <span class="special_format">Ferguson2024circle_line-value_delta</span>, to the Brain-Score registry via
        the dictionary on lines 9-22. For a benchmark plugin, the key in the dictionary is the benchmark name, and its
        value is a Benchmark object, which we will see soon.
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        You can see here that there are 14 benchmarks that are being added to the registry, and in the case of our
        <span class="special_format">Ferguson2024</span> benchmark, these are 14 "sub benchmarks" that each use a different
        <span class="special_format">StimulusSet</span> and <span class="special_format">BehavioralAssembly</span>
        (explained in the previous half of this tutorial series). These 14 benchmarks belong to a parent benchmark object,
        <span class="special_format">Ferguson2024</span>, and are called via the 14 lambda expressions with a different
        stimuli and data identifier. We also try to follow
        the naming convention of <span class="special_format">AuthorYEAR-metric</span>, as shown here.
    </p>

    <p class="benefits_info is-size-5-mobile shorter">
        Please note: Brain-Score does not allow duplicate plugin
        names, so if you submit another version of the same model, make sure to make the identifier unique!
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Also note: It would be prohibitively time- and resource-consuming to actually load every plugin
        in a registry before they are needed, so the plugin discovery process relies on string parsing.
        This unfortunately means that it’s not possible to programmatically add plugin identifiers to
        the registry; each registration needs to be written explicitly in the form of
        <span class="special_format">plugin_registry['my_plugin_identifier']</span>
        Next, let’s check out the second required file, <span class="special_format">test.py</span> file.
    </p>
</div>
<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 5: Exploring the test.py File</h3>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_test.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        Our <span class="special_format">test.py</span> file again serves the purpose of testing our benchmark to ensure
        scores are coming out as expected. There is a little bit of a circular dependency here, as you will not know
        scores until you actually run your benchmark; however, you can run 1-2 models locally, and hardcode what the values
        should be in your tests, which is what is done in the screenshot above.
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        On lines 58-71, you can see that each of the 14 benchmarks have an expected raw score associated with them that
        is passed into a pytest object. These scores were generated via a local call to Brain-Score, and you can learn more
        about how to do that by visiting the model tutorial
        <a href="http{% if request.is_secure %}s{% endif %}://{{ request.get_host }}/tutorials/models/quickstart">Quickstart</a>.

    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Line 73 defines the function <span class="special_format">test_model_raw_score</span>, and that is what is seen
        in lines 74-85; we are loading a benchmark and a precomputed model's features on that benchmark, and them comparing
        that to an expected score. Note: to do this successfully requires uploading precomputed model features to our S3
        bucket (however you can also host externally); please contact a Team Member if you need credentials to do this. For
        more information on how to generate precomputed model features for your tests, please see
        <a href="https://github.com/brain-score/vision/tree/master/tests#precomputed-features">here</a> (The
        "Precomputed features" section, and you can ignore steps 5 and 6 that are now outdated).
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        There are also other tests that are in the <span class="special_format">test.py</span> that are simply not shown
        in the screenshot above. These include tests for existence of the benchmark, tests to ensure the benchmark ceiling
        is working correctly, and tests to make sure the ceiled score is what it should be. Please feel free to base
        future submission tests off of the tests in this file!
    </p>
</div>
<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 6: Exploring the (optional) requirements.txt  File</h3>
    <img class="submission example" width="500" height="auto" src="{% static "/benchmarks/img/tutorials/benchmark_req.png" %}" />
    <p class="benefits_info is-size-5-mobile shorter">
        As with all Brain-Score plugins, you can use a <span class="special_format">requirements.txt</span> to define
        any needed dependencies. If you do not include any external packages used here, such as numpy, sympy, and pandas,
        then your benchmark will not be able to run on our servers (this will be caught in Github's PR tests). The reason
        that this is optional is that if your plugin does not need any external packages, then you do not need to submit
        this file.
    </p>

</div>
<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 7: Exploring the benchmark.py File</h3>
    <p class="benefits_info is-size-5-mobile shorter">
        This file is usually where the model itself is defined. The reason why this is optional is that using
        <span class="special_format">model.py</span> is a convention we use to
        separate code, but that is not needed. It is also plugin-specific. We think it is good practice,
        however, to use a separate <span class="special_format">model.py</span> for models, to keep things neat.
    </p>
    <p class="benefits_info is-size-5-mobile shorter">
        Let’s explore this file in more detail: Lines 1 - 9 are standard imports. Lines
        20-21 define the <span class="special_format">get_model_list()</span> function that simply returns the model identifier.
        Next, lines 24-30 define the <span class="special_format">get_model()</span> function that gets the model itself.
        You can see on line 26 the model itself is being loaded from <span class="special_format">pytorch</span>. If you have a custom
        model that you have created yourself, check out our Custom Model Submission Guide here in
        Deep Dive 2. Lines 33 - 35 contain the <span class="special_format">get_layers()</span> function, which returns the layers you
        are interested in scoring.
    </p>
    <pre class="modified_1"><code>
1       from brainscore_vision.model_helpers.check_submission import check_models
2       import functools
3       import os
4       import torchvision.models
5       from brainscore_vision.model_helpers.activations.pytorch import PytorchWrapper
6       from brainscore_vision.model_helpers.activations.pytorch import load_preprocess_images
7       from pathlib import Path
8       from brainscore_vision.model_helpers import download_weights
9       import torch
10
11      # This is an example implementation for submitting resnet-50 as a pytorch model
12
13      # Attention: It is important, that the wrapper identifier is unique per model!
14      # The results will otherwise be the same due to brain-scores internal result caching mechanism.
15      # Please load your pytorch model for usage in CPU. There won't be GPUs available for scoring your model.
16      # If the model requires a GPU, contact the brain-score team directly.
17      from brainscore_vision.model_helpers.check_submission import check_models
18
19
20      def get_model_list():
21          return ['resnet50']
22
23
24      def get_model(name):
25          assert name == 'resnet50'
26          model = torchvision.models.resnet50(pretrained=True)
27          preprocessing = functools.partial(load_preprocess_images, image_size=224)
28          wrapper = PytorchWrapper(identifier='resnet50', model=model, preprocessing=preprocessing)
29          wrapper.image_size = 224
30          return wrapper
31
32
33      def get_layers(name):
34          assert name == 'resnet50'
35          return ['conv1','layer1', 'layer2', 'layer3', 'layer4', 'fc']
36
37
38      def get_bibtex(model_identifier):
39          return """xx"""
40
41
42      if __name__ == '__main__':
43          check_models.check_base_models(__name__)

    </code></pre>
    <p class="benefits_info is-size-5-mobile shorter">
        Lines 38 and 39 define the Bibtex for the model. You can leave this blank when submitting,
        but we highly recommend you add a reference. Finally, lines 42 - 43 define a call to the
        Brain-Score scoring system to score our model locally on the <span class="special_format">MajajHong2015public.IT-pls</span>
        benchmark.
    </p>
</div>
<div class="box leaderboard-table-component">
    <h3 class="benefits_heading is-size-3-mobile">Part 8: Putting it All Together </h3>
    <p class="benefits_info is-size-5-mobile shorter">
        You are almost done! If you were actually submitting a model, the final step would be to run your model locally,
        to ensure that everything is in working order. You can do this by running the
        <span class="special_format">model.py</span> file itself. Please note that this can take ~5-10 minutes on a 2023
        M1 Max MacBook Pro, so your run times may vary.
        Once ran, this should produce the message below, indicating that you are ready for submission:
        <pre class="modified_1"><code>
Test successful, you are ready to submit!
    </code></pre>



    <div class="columns is-tablet is-variable is-1-tablet">
        <div class="column is-one-half tutorial_text">
           <p class="benefits_info is-size-5-mobile shorter top_adjust">
                Once you receive this message, you could rezip (after you save everything, of course) your package, and you
                would be good to submit your model.
           </p>
            <p class="benefits_info is-size-5-mobile shorter ">
                When you submit an actual (non-tutorial) model, you will receive an email with your results
                within 24 hours (most of the time it only takes 2-3 hours to score).
                If you would like to explore a custom model's submission package, please visit the next
                Deep Dive <a href="http{% if request.is_secure %}s{% endif %}://{{ request.get_host }}/tutorial/models/deepdive_2">here</a>.
            </p>
        </div>
        <div class="column is-one-half">
            <img class="tutorial_dd1_2" src="{% static "/benchmarks/img/tutorials/deep_dive_1_2.png" %}" />
        </div>
    </div>


</div>
{% endblock %}


